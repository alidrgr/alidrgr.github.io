{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"about-me/","title":"About me","text":"<p>I'm Ali, a graduate student of Computer Science at the University of Tehran.  My research mostly focuses on Natural Language Processing, but I'm also interested in AI Alignment, and Mechanistic Interpretability. Previously, I was a student of Computer Engineering at the Institute for Advanced Studies in Basic Sciences. </p>"},{"location":"logs/","title":"Logs","text":"<p>These are monthly logs of things that have piqued my interest.</p>"},{"location":"logs/#february-2025","title":"February 2025","text":"<ul> <li> <p>You probably have not thought about the complexity beneath your phone's calculator app. Well, here is an interesting thread about Hans Boehm's efforts in designing a calculator app for Android. There is also a paper<sup>6</sup> by Boehm himself. </p> </li> <li> <p>MIT CS 6.S184 is a course on denoising diffusion models and flow matching, with lecture notes available online.</p> </li> <li> <p>\"Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations\"<sup>7</sup> identifies two distinct mechanisms behind hallucinations in LLMs: Knowledge Enrichment hallucinations, occurring in early to middle MLP layers, and Answer Extraction hallucinations, caused by middle to late self-attention heads.</p> </li> </ul>"},{"location":"logs/#january-2025","title":"January 2025","text":"<ul> <li> <p>Escaping The Flatland is an interesting essay on career trajectory that describes how mimetic desires and peer pressure define and shape a few constrained 'planes of legibility' in the space of things one could consider viable and valuable in life.</p> </li> <li> <p>\"Attention Satisfies\"<sup>1</sup>, an ICLR 2024 paper, frames hallucination detection in LLMs as a constraint satisfaction problem and trains a probe on attention values. The idea of zero-knowledge hallucination detection in LLMs appears to be a promising avenue for further research. Two important baselines I've found in this line of research are \"SelfCheck-GPT\"<sup>2</sup> and \"SAPLMA (Internal States)\"<sup>3</sup>.</p> </li> <li> <p>The Theory Group at UC Berkeley has a pleasantly simple webpage, made even better by its archive of past course offerings.</p> </li> <li> <p>With DeepSeek-R1 surprising everyone with its performance, reading the report<sup>4</sup> felt essential. The DeepSeek-AI group had previously published another paper before DeepSeek-R1 named \"DeepSeekMath\"<sup>5</sup>, which I discovered thanks to a YouTube video by Yannic Kilcher.</p> </li> <li> <p>Solving Problems the Clojure Way is a well-structured talk. Watching a functional refactor in action while hearing the thought process behind it was quite clarifying.</p> </li> </ul>"},{"location":"logs/#references","title":"References","text":"<ol> <li> <p>M. Yuksekgonul et al., \u201cAttention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models,\u201d presented at the The Twelfth International Conference on Learning Representations, Oct. 2023. Accessed: Oct. 21, 2024. [Online]. Available: https://openreview.net/forum?id=gfFVATffPd\u00a0\u21a9</p> </li> <li> <p>P. Manakul, A. Liusie, and M. Gales, \u201cSelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,\u201d in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, H. Bouamor, J. Pino, and K. Bali, Eds., Singapore: Association for Computational Linguistics, Dec. 2023, pp. 9004\u20139017. doi: 10.18653/v1/2023.emnlp-main.557.\u00a0\u21a9</p> </li> <li> <p>A. Azaria and T. Mitchell, \u201cThe Internal State of an LLM Knows When It\u2019s Lying,\u201d in Findings of the Association for Computational Linguistics: EMNLP 2023, H. Bouamor, J. Pino, and K. Bali, Eds., Singapore: Association for Computational Linguistics, Dec. 2023, pp. 967\u2013976. doi: 10.18653/v1/2023.findings-emnlp.68.\u00a0\u21a9</p> </li> <li> <p>DeepSeek-AI et al., \u201cDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\u201d Jan. 22, 2025, arXiv: arXiv:2501.12948. doi: 10.48550/arXiv.2501.12948.\u00a0\u21a9</p> </li> <li> <p>Z. Shao et al., \u201cDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,\u201d Apr. 27, 2024, arXiv: arXiv:2402.03300. doi: 10.48550/arXiv.2402.03300.\u00a0\u21a9</p> </li> <li> <p>H.-J. Boehm, \u201cTowards an API for the real numbers,\u201d in Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation, in PLDI 2020. New York, NY, USA: Association for Computing Machinery, Jun. 2020, pp. 562\u2013576. doi: 10.1145/3385412.3386037.\u00a0\u21a9</p> </li> <li> <p>L. Yu, M. Cao, J. C. Cheung, and Y. Dong, \u201cMechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations,\u201d in Findings of the Association for Computational Linguistics: EMNLP 2024, Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, Eds., Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 7943\u20137956. doi: 10.18653/v1/2024.findings-emnlp.466.\u00a0\u21a9</p> </li> </ol>"},{"location":"Blog/","title":"Unconvex Drafts: a blog","text":""},{"location":"Blog/2025/and-yet-to-times-in-hope-my-verse-shall-stand/","title":"First Post","text":"<p>Writing never reaches a final state. Every paragraph can be rewritten. Every sentence can be rephrased. And, of course, every word can be swapped with a synonym. There are always a few rough edges to polish. So, I'd rather see these posts not as perfected writings but as published drafts. There is no optimal point for a text. You start with a few words, and after some iterations, end up with a text that looks not perfect but fine. That's why this blog is called \"Unconvex Drafts.\"</p>"},{"location":"Blog/archive/2025/","title":"2025","text":""},{"location":"Blog/category/misc/","title":"misc","text":""}]}